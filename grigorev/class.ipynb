{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Detected that PyTorch and torch_sparse were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_sparse has CUDA version 0.0. Please reinstall the torch_sparse that matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6144a933a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGMMConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet2Set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataListLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[1;32m      9\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt_minor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;34mf'Detected that PyTorch and torch_sparse were compiled with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;34mf'different CUDA versions. PyTorch has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34mf'{t_major}.{t_minor} and torch_sparse has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torch_sparse were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_sparse has CUDA version 0.0. Please reinstall the torch_sparse that matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto  import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
    "from torch_geometric.data import (InMemoryDataset, Data)\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)\n",
    "\n",
    "from torch_geometric.datasets import MNISTSuperpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: torch: not found\r\n"
     ]
    }
   ],
   "source": [
    "!torch --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSkeleton(InMemoryDataset):\n",
    "    r\"\"\"The skeleton on MNIST dataset\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, root, dataset=\"train\", transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(MNISTSkeleton, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        if dataset == \"train\":\n",
    "            path = self.processed_paths[0]\n",
    "        elif dataset == \"test\":\n",
    "            path = self.processed_paths[1]\n",
    "        elif dataset == \"val\":\n",
    "            path = self.processed_paths[2]\n",
    "        self.data, self.slices = torch.load(path)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train', 'test', 'val']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train_prc', 'test_prc', 'val_prc']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def get_edges_ged_rad(self, skel_features):\n",
    "        edges_x = []\n",
    "        edges_y = []\n",
    "        degs = set()\n",
    "        rads = set()\n",
    "        for i, d in enumerate(skel_features):\n",
    "            if i % 4 == 0:\n",
    "                edges_x.append(d)\n",
    "            if i % 4 == 1:\n",
    "                edges_y.append(d)\n",
    "            if i % 4 == 2:\n",
    "                degs.add((edges_x[-1], edges_y[-1], d))\n",
    "            if i % 4 == 3:\n",
    "                rads.add((edges_x[-1], edges_y[-1], d))\n",
    "    \n",
    "        return edges_x, edges_y, list(degs), list(rads)\n",
    "    \n",
    "    def get_dataset(self, skel_features):\n",
    "        pos = []\n",
    "        features = []\n",
    "        node_slice = [0]\n",
    "        edge_index = []\n",
    "        edge_slice = [0]\n",
    "        node_count = 0\n",
    "        edge_global_count = 0\n",
    "                \n",
    "        for x in skel_features:\n",
    "            xx, yy, deg, rad = self.get_edges_ged_rad(x)\n",
    "            skeleton = {}\n",
    "            n = 0\n",
    "            for i in range(len(deg)):\n",
    "                x, y, d = deg[i]\n",
    "                if i >= len(rad):\n",
    "                    r = 0\n",
    "                else:\n",
    "                    _, _, r = rad[i]\n",
    "                skeleton[(x, y)] = n\n",
    "                n += 1\n",
    "                pos.append([x, y])\n",
    "                features.append([d, r])\n",
    "                \n",
    "            node_count += len(deg)\n",
    "            node_slice.append(node_count)\n",
    "            for i in range(0, len(xx), 2):\n",
    "                if (xx[i], yy[i]) in skeleton and (xx[i+1], yy[i+1]) in skeleton:\n",
    "                    edge_index.append([skeleton[(xx[i], yy[i])], skeleton[(xx[i+1], yy[i+1])]]) \n",
    "                    edge_global_count += 1\n",
    "            edge_slice.append(edge_global_count)\n",
    "            \n",
    "        return torch.tensor(pos), torch.tensor(features), torch.tensor(node_slice), torch.tensor(edge_index).t().contiguous(), torch.tensor(edge_slice)\n",
    "        \n",
    "    def process(self):\n",
    "        for raw_path, path in zip(self.raw_paths, self.processed_paths):\n",
    "            with open(raw_path, \"rb\") as fin:\n",
    "                data = pickle.load(fin)\n",
    "                Y, X_skel_features = data[\"labels\"], data[\"skel_features\"]\n",
    "            pos, features, node_slice, edge_index, edge_slice = self.get_dataset(X_skel_features)            \n",
    "            node_slice, Y = node_slice.to(torch.long), torch.as_tensor(Y).to(torch.long)\n",
    "            graph_slice = torch.arange(Y.size(0,)+1)\n",
    "            self.data = Data(x=features, y=Y, pos=pos, edge_index=edge_index)\n",
    "            self.slices = {\n",
    "                'x': node_slice,\n",
    "                'y': graph_slice,\n",
    "                'pos': node_slice,\n",
    "                'edge_index': edge_slice\n",
    "            }\n",
    "\n",
    "            if self.pre_filter is not None:\n",
    "                data_list = [self.get(idx) for idx in range(len(self))]\n",
    "                data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "                self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data_list = [self.get(idx) for idx in range(len(self))]\n",
    "                data_list = [self.pre_transform(data) for data in data_list]\n",
    "                self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "            torch.save((self.data, self.slices), path)\n",
    "\n",
    "\n",
    "class MoNet(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MoNet, self).__init__()\n",
    "        self.conv1 = GMMConv(in_channels=num_features, out_channels=32, dim=2)\n",
    "        self.batchnorm_1 = torch.nn.BatchNorm1d(32)\n",
    "        self.conv2 = GMMConv(in_channels=32, out_channels=64, dim=2)\n",
    "        self.batchnorm_2 = torch.nn.BatchNorm1d(64)\n",
    "        self.conv3 = GMMConv(in_channels=64, out_channels=64, dim=2)\n",
    "        self.batchnorm_3 = torch.nn.BatchNorm1d(64)\n",
    "        self.fc1 = torch.nn.Linear(64, 80)\n",
    "        self.fc2 = torch.nn.Linear(80, 10)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data.x = F.elu(self.batchnorm_1(self.conv1(data.x, data.edge_index, data.edge_attr)))\n",
    "       \n",
    "        data.x = F.elu(self.batchnorm_2(self.conv2(data.x, data.edge_index, data.edge_attr)))\n",
    "\n",
    "        data.x = F.elu(self.batchnorm_3(self.conv3(data.x, data.edge_index, data.edge_attr)))\n",
    "\n",
    "        x = global_mean_pool(data.x, data.batch)\n",
    "\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "      \n",
    "\n",
    "class MPNNNet(nn.Module):\n",
    "    def __init__(self, num_features, aggr='mean', processing_steps=8, message_passing_steps=4, dim=64):\n",
    "        super(MPNNNet, self).__init__()\n",
    "        self.message_passing_steps = message_passing_steps\n",
    "        \n",
    "        self.lin0 = torch.nn.Linear(num_features, dim) # Change 2 to 1 for superpixels\n",
    "        nn = torch.nn.Sequential(torch.nn.Linear(2, 128), torch.nn.ReLU(), torch.nn.Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr=aggr, root_weight=False)\n",
    "        self.gru = torch.nn.GRU(dim, dim)\n",
    "        \n",
    "        self.set2set = Set2Set(dim, processing_steps=processing_steps)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = F.relu(self.lin0(data.x))\n",
    "        h = out.unsqueeze(0)\n",
    "\n",
    "        for i in range(self.message_passing_steps):\n",
    "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        return F.log_softmax(self.lin2(out), dim=1)\n",
    "\n",
    "class SplineCNN(nn.Module):\n",
    "    def __init__(self, num_features, kernel=5, dim=2, num_classes=10):\n",
    "        super(SplineCNN, self).__init__()\n",
    "        self.conv1 = SplineConv(num_features, 32, dim, kernel)\n",
    "        self.conv2 = SplineConv(32, 64, dim, kernel)\n",
    "        self.fc1 = torch.nn.Linear(64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = F.elu(self.conv1(data.x, data.edge_index, data.edge_attr))\n",
    "\n",
    "        data.x = F.elu(self.conv2(data.x, data.edge_index, data.edge_attr))\n",
    "    \n",
    "        x = global_mean_pool(data.x, data.batch)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        return F.log_softmax(self.fc2(x), dim=1)\n",
    "     \n",
    "    \n",
    "class GNNNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes=10):\n",
    "        super(GNNNet, self).__init__()\n",
    "        self.conv1 = GraphConv(num_features, 32)\n",
    "        self.conv2 = GraphConv(32, 64)\n",
    "        self.conv3 = GraphConv(64, 64)\n",
    "        self.fc1 = torch.nn.Linear(64, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, num_classes)\n",
    " \n",
    "    def reset_parameters(self):\n",
    "        for (name, module) in self._modules.items():\n",
    "            module.reset_parameters()\n",
    " \n",
    "    def forward(self, data):\n",
    "        data.x = F.elu(self.conv1(data.x, data.edge_index))\n",
    "        data.x = F.elu(self.conv2(data.x, data.edge_index))\n",
    "        data.x = F.elu(self.conv3(data.x, data.edge_index))\n",
    "        x_1 = scatter_mean(data.x, data.batch, dim=0)\n",
    "        x = x_1\n",
    " \n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def get_train_val_loader(path, train_batch_size=64, val_batch_size=64, val_split=1/12):\n",
    "    train_dataset = MNISTSuperpixels(path, \"train\", transform=T.Cartesian())\n",
    "    dataset_size = len(train_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(val_split * dataset_size)\n",
    "    np.random.seed(43)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[split:]\n",
    "    val_indices = indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    validation_loader = DataLoader(train_dataset, batch_size=val_batch_size,\n",
    "                                                sampler=val_sampler, shuffle=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, \n",
    "                                                sampler=train_sampler, shuffle=False)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "\n",
    "def train(epoch, model, train_loader, device, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for data in tqdm(train_loader, leave=False):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        responce = model(data)\n",
    "        F.nll_loss(responce, data.y).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def validate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(test_loader.sampler)\n",
    "\n",
    "\n",
    "def process_model(network, out_file_name, train_loader, validation_loader, test_loader,\n",
    "                  init_lr=0.01, num_epochs=150):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = network(train_loader.dataset.num_features).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.8, patience=5, min_lr=0.00001, verbose=True)\n",
    "\n",
    "    with open(out_file_name, 'w') as file:\n",
    "      start_time = time.time()\n",
    "      for epoch in tqdm(range(num_epochs)):\n",
    "        train(epoch, model, train_loader, device, optimizer)\n",
    "        test_acc = validate(model, validation_loader, device)\n",
    "        scheduler.step(test_acc)\n",
    "        print('Epoch: {:02d}, Time: {:.4f}, Validation Accuracy: {:.4f}'\\\n",
    "              .format(epoch, time.time() - start_time, test_acc), file=file)\n",
    "\n",
    "      start_time = time.time()\n",
    "      test_acc = validate(model, test_loader, device)\n",
    "      print('Test, Time: {:.4f}, Accuracy: {:.4f}'\\\n",
    "            .format(time.time() - start_time, test_acc), file=file)\n",
    " \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # SuperPixels dataset\n",
    "    sp_path = os.path.join(os.path.dirname(os.path.realpath(\"/\")), \"MNISTSuperpixel\")\n",
    "    sp_test_dataset = MNISTSuperpixels(sp_path, train=False, transform=T.Cartesian())\n",
    "    sp_train_loader_25, sp_val_loader_25 = get_train_val_loader(sp_path, train_batch_size=25, val_batch_size=25)\n",
    "    sp_train_loader_64, sp_val_loader_64 = get_train_val_loader(sp_path, train_batch_size=64, val_batch_size=64)\n",
    "    sp_test_loader = DataLoader(sp_test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Skeletons dataset\n",
    "    sk_path = \"dataset\"\n",
    "    sk_train_dataset = MNISTSkeleton(sk_path, \"train\", transform=T.Polar())\n",
    "    sk_test_dataset = MNISTSkeleton(sk_path, \"test\", transform=T.Polar())\n",
    "    sk_val_dataset = MNISTSkeleton(sk_path, \"val\", transform=T.Polar())\n",
    "\n",
    "    sk_train_loader = DataLoader(sk_train_dataset, batch_size=64, shuffle=True)\n",
    "    sk_val_loader = DataLoader(sk_val_dataset, batch_size=64, shuffle=False)\n",
    "    sk_test_loader = DataLoader(sk_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train\n",
    "    num_epochs = 150\n",
    "    print('MoNet Superpixel starts:')\n",
    "    process_model(MoNet, 'MoNet_SuperPixels.txt', sp_train_loader_25, sp_val_loader_25, sp_test_loader, init_lr=0.01, num_epochs=num_epochs)\n",
    "    print('MoNet Skeletons starts:')\n",
    "    process_model(MoNet, 'MoNet_Skeletons.txt', sk_train_loader, sk_val_loader, sk_test_loader, init_lr=0.01, num_epochs=num_epochs)\n",
    "\n",
    "    print('MPNNNet Superpixel starts:')\n",
    "    process_model(MPNNNet, 'MPNNNet_SuperPixels.txt', sp_train_loader_64, sp_val_loader_64, sp_test_loader, init_lr=0.001, num_epochs=num_epochs)\n",
    "    print('MPNNNet Skeletons starts:')\n",
    "    process_model(MPNNNet, 'MPNNNet_Skeletons.txt', sk_train_loader, sk_val_loader, sk_test_loader, init_lr=0.001, num_epochs=num_epochs)\n",
    "\n",
    "    print('GNNNet Superpixel starts:')\n",
    "    process_model(GNNNet, 'GNNNet_SuperPixels.txt', sp_train_loader_64, sp_val_loader_64, sp_test_loader, init_lr=0.001, num_epochs=num_epochs)\n",
    "    print('GNNNet Skeletons starts:')\n",
    "    process_model(GNNNet, 'GNNNet_Skeletons.txt', sk_train_loader, sk_val_loader, sk_test_loader, init_lr=0.001, num_epochs=num_epochs)\n",
    "\n",
    "    print('SplineCNN Superpixel starts:')\n",
    "    process_model(SplineCNN, 'SplineCNN_SuperPixels.txt', sp_train_loader_64, sp_val_loader_64, sp_test_loader, init_lr=0.01, num_epochs=num_epochs)\n",
    "    print('SplineCNN Skeletons starts:')\n",
    "    process_model(SplineCNN, 'SplineCNN_Skeletons.txt', sk_train_loader, sk_val_loader, sk_test_loader, init_lr=0.01, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
